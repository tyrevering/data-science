{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ca37e2",
   "metadata": {},
   "source": [
    "# Building Machine Learning Pipelines using PySpark\n",
    "- Spark is a framework for working with Big Data, and has it's own machine learning libraries and workflow\n",
    "- Using the libraries and functions available through the PySpark module, this project aims to portray a classic Logistic Regression workflow in PySpark exploring a dataset on the English Football Premier League"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1548c163",
   "metadata": {},
   "source": [
    "## Starting Spark Session\n",
    "- Vital for interacting with Spark's functionalities, such as distributed computing execution, and the use of SQL operations for preprocessing \n",
    "- The use of these functionalities allows for more scalabale and efficient data processing for machine learning pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f60a4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "                    .master('local[1]') \\\n",
    "                    .appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3fe336",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "- Using Spark's DataFrame API, reading CSV files is simple, but defining the data's `schema` is important for feature engineering and model training\n",
    "- As one can see, when the `schema` is inferred, every column is a `StringType`, which can be adjusted by defining a schema using `pyspark.sql.types`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "246edd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- club: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- position: string (nullable = true)\n",
      " |-- position_cat: string (nullable = true)\n",
      " |-- market_value: string (nullable = true)\n",
      " |-- page_views: string (nullable = true)\n",
      " |-- fpl_value: string (nullable = true)\n",
      " |-- fpl_sel: string (nullable = true)\n",
      " |-- fpl_points: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- nationality: string (nullable = true)\n",
      " |-- new_foreign: string (nullable = true)\n",
      " |-- age_cat: string (nullable = true)\n",
      " |-- club_id: string (nullable = true)\n",
      " |-- big_club: string (nullable = true)\n",
      " |-- new_signing: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv('epldata_final.csv', header=True) # loading data\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608cf635",
   "metadata": {},
   "source": [
    "### Schema Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef542f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- club: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- position: string (nullable = true)\n",
      " |-- position_cat: integer (nullable = true)\n",
      " |-- market_value: double (nullable = true)\n",
      " |-- page_views: integer (nullable = true)\n",
      " |-- fpl_value: double (nullable = true)\n",
      " |-- fpl_sel: double (nullable = true)\n",
      " |-- fpl_points: integer (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- nationality: string (nullable = true)\n",
      " |-- new_foreign: integer (nullable = true)\n",
      " |-- age_cat: integer (nullable = true)\n",
      " |-- club_id: integer (nullable = true)\n",
      " |-- big_club: integer (nullable = true)\n",
      " |-- new_signing: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "defined_schema = StructType([StructField('name', StringType()), # defining schema\n",
    "                             StructField('club', StringType()),\n",
    "                             StructField('age', IntegerType()),\n",
    "                             StructField('position', StringType()),\n",
    "                             StructField('position_cat', IntegerType()),\n",
    "                             StructField('market_value', DoubleType()),\n",
    "                             StructField('page_views',  IntegerType()),\n",
    "                             StructField('fpl_value', DoubleType()),\n",
    "                             StructField('fpl_sel', DoubleType()),\n",
    "                             StructField('fpl_points', IntegerType()),\n",
    "                             StructField('region', StringType()),\n",
    "                             StructField('nationality', StringType()),\n",
    "                             StructField('new_foreign', IntegerType()),\n",
    "                             StructField('age_cat', IntegerType()),\n",
    "                             StructField('club_id', IntegerType()),\n",
    "                             StructField('big_club', IntegerType()),\n",
    "                             StructField('new_signing', IntegerType())])\n",
    "\n",
    "data = spark.read.csv('epldata_final.csv', header=True, schema=defined_schema)\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7e35c1",
   "metadata": {},
   "source": [
    "### Dropping unneeded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5657800e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name',\n",
       " 'club',\n",
       " 'age',\n",
       " 'position',\n",
       " 'position_cat',\n",
       " 'market_value',\n",
       " 'page_views',\n",
       " 'fpl_value',\n",
       " 'fpl_sel',\n",
       " 'fpl_points',\n",
       " 'region',\n",
       " 'nationality',\n",
       " 'club_id']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(*['big_club', 'new_signing', 'new_foreign', 'age_cat']) # dropping unneeded columns\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2ce873",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "- Using Spark's specific querying functions, one can explore a dataset thorougly to understand the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23f8866a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(461, 13)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count(), len(data.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "397f4cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-----------------+\n",
      "|summary|      market_value|         fpl_value|       page_views|\n",
      "+-------+------------------+------------------+-----------------+\n",
      "|  count|               461|               461|              461|\n",
      "|   mean|11.012039045553143| 5.447939262472885|763.7765726681127|\n",
      "| stddev|12.257402881461974|1.3466952645024657| 931.805757407049|\n",
      "|    min|              0.05|               4.0|                3|\n",
      "|    max|              75.0|              12.5|             7664|\n",
      "+-------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('market_value', 'fpl_value', 'page_views').describe().show() # exploring numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86242c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|             club|count|\n",
      "+-----------------+-----+\n",
      "|        Tottenham|   20|\n",
      "|Brighton+and+Hove|   22|\n",
      "|         West+Ham|   22|\n",
      "|   Leicester+City|   24|\n",
      "|          Arsenal|   28|\n",
      "|Manchester+United|   25|\n",
      "|        West+Brom|   19|\n",
      "|          Burnley|   18|\n",
      "|      Bournemouth|   24|\n",
      "| Newcastle+United|   21|\n",
      "|      Southampton|   23|\n",
      "|          Swansea|   25|\n",
      "|     Huddersfield|   28|\n",
      "|   Crystal+Palace|   21|\n",
      "|        Liverpool|   27|\n",
      "|          Chelsea|   20|\n",
      "|  Manchester+City|   20|\n",
      "|          Watford|   24|\n",
      "|          Everton|   28|\n",
      "|       Stoke+City|   22|\n",
      "+-----------------+-----+\n",
      "\n",
      "+-------------+-----+\n",
      "|  nationality|count|\n",
      "+-------------+-----+\n",
      "|      Senegal|    7|\n",
      "|       Sweden|    3|\n",
      "|      Germany|   16|\n",
      "|       France|   25|\n",
      "|       Greece|    1|\n",
      "|     Congo DR|    4|\n",
      "|      Algeria|    3|\n",
      "|        Wales|   12|\n",
      "|    Argentina|   17|\n",
      "|      Belgium|   18|\n",
      "|      Ecuador|    2|\n",
      "|      Finland|    1|\n",
      "|        Ghana|    5|\n",
      "|   The Gambia|    1|\n",
      "|        Benin|    1|\n",
      "|      Curacao|    1|\n",
      "|United States|    2|\n",
      "|       Bosnia|    2|\n",
      "|        Chile|    2|\n",
      "|      Croatia|    1|\n",
      "+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy('club').count().show() # group by statement\n",
    "data.groupBy('nationality').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72c1c321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---+--------+------------+------------+----------+---------+-------+----------+------+-----------+-------+\n",
      "|name|club|age|position|position_cat|market_value|page_views|fpl_value|fpl_sel|fpl_points|region|nationality|club_id|\n",
      "+----+----+---+--------+------------+------------+----------+---------+-------+----------+------+-----------+-------+\n",
      "|   0|   0|  0|       0|           0|           0|         0|        0|    461|         0|     0|          0|      0|\n",
      "+----+----+---+--------+------------+------------+----------+---------+-------+----------+------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as f # using sql functionality for null value check\n",
    "\n",
    "aggregated_data = data.agg(*[f.count(f.when(f.isnull(c), c)).alias(c) for c in data.columns])\n",
    "aggregated_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673b7213",
   "metadata": {},
   "source": [
    "## String Indexing\n",
    "- `StringIndexer()`will convert the categorical values of this dataset, which allows a logistic regression model to interpret categorical variables as numerical feautures during training processes of machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89892b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer # string indexing\n",
    "\n",
    "si_name = StringIndexer(inputCol='name', outputCol='id_name')\n",
    "si_club = StringIndexer(inputCol='club', outputCol='id_club')\n",
    "si_nation = StringIndexer(inputCol='nationality', outputCol='id_nationality')\n",
    "\n",
    "data = si_name.fit(data).transform(data)\n",
    "data = si_club.fit(data).transform(data)\n",
    "data = si_nation.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b52934f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------+-------+-------+--------------+--------------+\n",
      "|             name|id_name|   club|id_club|   nationality|id_nationality|\n",
      "+-----------------+-------+-------+-------+--------------+--------------+\n",
      "|   Alexis Sanchez|   19.0|Arsenal|    0.0|         Chile|          31.0|\n",
      "|       Mesut Ozil|  302.0|Arsenal|    0.0|       Germany|           7.0|\n",
      "|        Petr Cech|  348.0|Arsenal|    0.0|Czech Republic|          32.0|\n",
      "|     Theo Walcott|  418.0|Arsenal|    0.0|       England|           0.0|\n",
      "|Laurent Koscielny|  250.0|Arsenal|    0.0|        France|           2.0|\n",
      "+-----------------+-------+-------+-------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('name', 'id_name', 'club', 'id_club', 'nationality', 'id_nationality').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daddd5db",
   "metadata": {},
   "source": [
    "## One Hot Encoding\n",
    "- `OneHotEncoder()` is another method in which PySpark uses to convert categorical variables into numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aff01f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder # one hot encoding\n",
    "\n",
    "OHE = OneHotEncoder(inputCols=['id_name', 'id_club', 'id_nationality'],\n",
    "                   outputCols=['ohe_name', 'ohe_club', 'ohe_nationality'])\n",
    "\n",
    "data = OHE.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce371cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------+-----------------+\n",
      "|             name|id_name|         ohe_name|\n",
      "+-----------------+-------+-----------------+\n",
      "|   Alexis Sanchez|   19.0| (460,[19],[1.0])|\n",
      "|       Mesut Ozil|  302.0|(460,[302],[1.0])|\n",
      "|        Petr Cech|  348.0|(460,[348],[1.0])|\n",
      "|     Theo Walcott|  418.0|(460,[418],[1.0])|\n",
      "|Laurent Koscielny|  250.0|(460,[250],[1.0])|\n",
      "+-----------------+-------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+-------+--------------+\n",
      "|   club|id_club|      ohe_club|\n",
      "+-------+-------+--------------+\n",
      "|Arsenal|    0.0|(19,[0],[1.0])|\n",
      "|Arsenal|    0.0|(19,[0],[1.0])|\n",
      "|Arsenal|    0.0|(19,[0],[1.0])|\n",
      "|Arsenal|    0.0|(19,[0],[1.0])|\n",
      "|Arsenal|    0.0|(19,[0],[1.0])|\n",
      "+-------+-------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------+--------------+---------------+\n",
      "|   nationality|id_nationality|ohe_nationality|\n",
      "+--------------+--------------+---------------+\n",
      "|         Chile|          31.0|(60,[31],[1.0])|\n",
      "|       Germany|           7.0| (60,[7],[1.0])|\n",
      "|Czech Republic|          32.0|(60,[32],[1.0])|\n",
      "|       England|           0.0| (60,[0],[1.0])|\n",
      "|        France|           2.0| (60,[2],[1.0])|\n",
      "+--------------+--------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('name', 'id_name', 'ohe_name').show(5)\n",
    "data.select('club', 'id_club', 'ohe_club').show(5)\n",
    "data.select('nationality', 'id_nationality', 'ohe_nationality').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b6f277",
   "metadata": {},
   "source": [
    "## Vector Assembler\n",
    "- The `VectorAssembler()` in PySpark asserts all the numerical values into a single column. This helps assemble feature vectors for machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46803311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              vector|\n",
      "+--------------------+\n",
      "|(544,[0,1,2,22,46...|\n",
      "|(544,[0,1,2,305,4...|\n",
      "|(544,[0,1,2,351,4...|\n",
      "|(544,[0,1,2,421,4...|\n",
      "|(544,[0,1,2,253,4...|\n",
      "|(544,[0,1,2,162,4...|\n",
      "|(544,[0,1,2,339,4...|\n",
      "|(544,[0,1,2,325,4...|\n",
      "|(544,[0,1,2,402,4...|\n",
      "|(544,[0,1,2,19,46...|\n",
      "|(544,[0,1,2,153,4...|\n",
      "|(544,[0,1,2,20,46...|\n",
      "|(544,[0,1,2,176,4...|\n",
      "|(544,[0,1,2,6,464...|\n",
      "|(544,[0,1,2,138,4...|\n",
      "|(544,[0,1,2,141,4...|\n",
      "|(544,[0,1,2,244,4...|\n",
      "|(544,[0,1,2,388,4...|\n",
      "|(544,[0,1,2,99,46...|\n",
      "|(544,[0,1,2,365,4...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler # vectorizing data\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['market_value',\n",
    "                                       'fpl_value',\n",
    "                                       'id_name',\n",
    "                                       'ohe_name',\n",
    "                                       'id_club',\n",
    "                                       'ohe_club',\n",
    "                                       'id_nationality',\n",
    "                                       'ohe_nationality'],\n",
    "                            outputCol='vector')\n",
    "\n",
    "data = data.fillna(0)\n",
    "final_data = assembler.transform(data)\n",
    "\n",
    "final_data.select('vector').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378a72b2",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "- Now that the data is all vectorized, you can perform a `LogisticRegression()`\n",
    "- The model will aim to predict the `id_name` column given the `vector` column from the `VectorAssembler()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e45eb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = final_data.randomSplit([.8, .2], seed=1) # train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d33dc6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+----------+--------------------+\n",
      "|              vector|id_name|       rawPrediction|prediction|         probability|\n",
      "+--------------------+-------+--------------------+----------+--------------------+\n",
      "|(544,[0,1,2,7,463...|    4.0|[0.24329764999350...|     457.0|[0.00148177657594...|\n",
      "|(544,[0,1,2,16,46...|   13.0|[0.18028050849506...|     115.0|[0.00178395079154...|\n",
      "|(544,[0,1,2,17,46...|   14.0|[0.32276799015208...|     265.0|[1.47898190919739...|\n",
      "|(544,[0,1,2,18,46...|   15.0|[0.31027327143040...|     265.0|[1.47278801917426...|\n",
      "|(544,[0,1,2,20,46...|   17.0|[0.56966790497067...|      96.0|[0.00287209348982...|\n",
      "|(544,[0,1,2,32,46...|   29.0|[-0.0083534992384...|       2.0|[0.00162949064057...|\n",
      "|(544,[0,1,2,35,46...|   32.0|[0.08215885074565...|     343.0|[0.00106303727317...|\n",
      "|(544,[0,1,2,36,46...|   33.0|[0.23828420684094...|     124.0|[8.28560608597087...|\n",
      "|(544,[0,1,2,45,46...|   42.0|[0.14654659750553...|     389.0|[8.06873209394717...|\n",
      "|(544,[0,1,2,47,46...|   44.0|[0.22085430514690...|     198.0|[0.00133469583765...|\n",
      "|(544,[0,1,2,54,46...|   51.0|[0.07732477442436...|      40.0|[0.00100816006404...|\n",
      "|(544,[0,1,2,57,46...|   54.0|[0.13897254737533...|     116.0|[8.77494550860216...|\n",
      "|(544,[0,1,2,61,46...|   58.0|[0.17184390901700...|      78.0|[0.00133495571876...|\n",
      "|(544,[0,1,2,68,46...|   65.0|[0.55896265400972...|     136.0|[0.00289824982380...|\n",
      "|(544,[0,1,2,77,46...|   74.0|[0.09773426513345...|     283.0|[6.08859442268012...|\n",
      "|(544,[0,1,2,78,46...|   75.0|[0.05388160986277...|      69.0|[3.15669345516174...|\n",
      "|(544,[0,1,2,86,46...|   83.0|[0.09627345684969...|      68.0|[9.60481651245716...|\n",
      "|(544,[0,1,2,87,46...|   84.0|[0.66654992021083...|      48.0|[0.00270917435858...|\n",
      "|(544,[0,1,2,91,46...|   88.0|[0.15245607236582...|     237.0|[0.00126897613958...|\n",
      "|(544,[0,1,2,95,46...|   92.0|[0.54602884773597...|      46.0|[0.00292968735361...|\n",
      "+--------------------+-------+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression # Logistic Regression\n",
    "\n",
    "logit = LogisticRegression(featuresCol='vector', labelCol='id_name').fit(data_train)\n",
    "\n",
    "prediction = logit.transform(data_test)\n",
    "prediction.select('vector','id_name', 'rawPrediction',\n",
    "                  'prediction', 'probability').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac8794c",
   "metadata": {},
   "source": [
    "## Machine Learning Pipeline\n",
    "- Using the same machine learning workflow as above, one can make a `Pipeline()`\n",
    "- Using the `stages` in the code section below, it shows how through the use of PySpark's distributed computing abilities, you can deploy scalable pipelines that can perform simple Machine Learning tasks on big data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38b256f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipe_data = spark.read.csv('epldata_final.csv', header=True, schema=defined_schema)\n",
    "stage_1 = StringIndexer(inputCol='name', outputCol='id_name_pipe')\n",
    "stage_2 = StringIndexer(inputCol='club', outputCol='id_club_pipe')\n",
    "stage_3 = StringIndexer(inputCol='nationality', outputCol='id_nationality_pipe')\n",
    "stage_4 = OneHotEncoder(inputCols=['id_name_pipe', 'id_club_pipe', 'id_nationality_pipe'],\n",
    "                   outputCols=['ohe_name_pipe', 'ohe_club_pipe', 'ohe_nationality_pipe'])\n",
    "stage_5 = VectorAssembler(inputCols=['market_value',\n",
    "                                       'fpl_value',\n",
    "                                       'id_name_pipe',\n",
    "                                       'ohe_name_pipe',\n",
    "                                       'id_club_pipe',\n",
    "                                       'ohe_club_pipe',\n",
    "                                       'id_nationality_pipe',\n",
    "                                       'ohe_nationality_pipe'],\n",
    "                            outputCol='vector')\n",
    "stage_6 = LogisticRegression(featuresCol='vector', labelCol='id_name_pipe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a64bf3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+----------+--------------------+\n",
      "|              vector|id_name_pipe|       rawPrediction|prediction|         probability|\n",
      "+--------------------+------------+--------------------+----------+--------------------+\n",
      "|(453,[0,1,3,378,3...|         0.0|[43.5902413358697...|       0.0|[1.0,1.8295883451...|\n",
      "|(453,[0,1,2,4,379...|         1.0|[0.43891107242437...|       1.0|[1.88641726789220...|\n",
      "|(453,[0,1,2,5,378...|         2.0|[-0.0107105684804...|       2.0|[6.43543400427312...|\n",
      "|(453,[0,1,2,6,378...|         3.0|[0.01211729401848...|       3.0|[3.49137053052877...|\n",
      "|(453,[0,1,2,7,378...|         4.0|[0.02632240996252...|       4.0|[7.19104370614431...|\n",
      "|(453,[0,1,2,8,378...|         5.0|[0.33565779776431...|       5.0|[1.36569827957330...|\n",
      "|(453,[0,1,2,9,378...|         6.0|[0.42580820882416...|       6.0|[1.81296644406015...|\n",
      "|(453,[0,1,2,10,37...|         7.0|[0.42895177708633...|       7.0|[1.88424604086132...|\n",
      "|(453,[0,1,2,11,37...|         8.0|[1.89925324852990...|       8.0|[4.34213469043873...|\n",
      "|(453,[0,1,2,12,37...|         9.0|[-0.0614206649699...|       9.0|[2.92119429731466...|\n",
      "|(453,[0,1,2,13,37...|        10.0|[-0.1206799498166...|      10.0|[7.74822463763419...|\n",
      "|(453,[0,1,2,14,37...|        11.0|[-0.0324318482051...|      11.0|[3.30221499627635...|\n",
      "|(453,[0,1,2,15,37...|        12.0|[0.01923197799902...|      12.0|[3.52161900829607...|\n",
      "|(453,[0,1,2,16,37...|        13.0|[-0.0647296093984...|      13.0|[3.41969033932427...|\n",
      "|(453,[0,1,2,17,37...|        14.0|[-0.0594234612930...|      14.0|[8.14529233518714...|\n",
      "|(453,[0,1,2,18,37...|        15.0|[0.34472536007097...|      15.0|[1.56024937456135...|\n",
      "|(453,[0,1,2,19,37...|        16.0|[-0.0824962164355...|      16.0|[1.04008932890960...|\n",
      "|(453,[0,1,2,20,37...|        17.0|[-0.0426691011451...|      17.0|[5.62790135363894...|\n",
      "|(453,[0,1,2,21,37...|        18.0|[1.79072612842262...|      18.0|[3.47782824033546...|\n",
      "|(453,[0,1,2,22,37...|        19.0|[0.23713587604660...|      19.0|[1.10140977874617...|\n",
      "+--------------------+------------+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regression_pipeline = Pipeline(stages=[stage_1, stage_2, stage_3, stage_4, stage_5, stage_6])\n",
    "pipe_train, pipe_test = pipe_data.randomSplit([0.8,0.2], seed=1)\n",
    "\n",
    "model = regression_pipeline.fit(pipe_train)\n",
    "pipe_train = model.transform(pipe_train)\n",
    "\n",
    "pipe_train.select('vector','id_name_pipe', 'rawPrediction',\n",
    "                  'prediction', 'probability').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
